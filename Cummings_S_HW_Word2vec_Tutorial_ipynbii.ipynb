{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Word Embedding using Word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Word2vec:\n",
    "\n",
    "* Is not a single algorithm, but rather a software package that represent words as vectors. Created by team of researchers at Google led by Tomas Mikolov it consists of two distinct models ( CBOW and Skip grams) see below. \n",
    "* It has various training methods like: Negative Sampling and Hieriachical Softmax.\n",
    "* the model and training most correlated are Skip gram and Negative Sampling. \n",
    "* Has a rich preprocessing pipeline consisting of: dynamic context windows, subsampling, and deleting rae words\n",
    "* Is a particularly computationally-efficient predictive model for learning word embeddings from raw text\n",
    "* Is a two-layer neural net that processes text.\n",
    "* Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus\n",
    "* It is not a deep neural network, and is used to turn text into a numerical form that deep nets can understand. \n",
    "* In addition to parsing sentences, the application can be applied to genes, code, likes, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned.\n",
    "\n",
    "* Why? Well since words simply discrete states, were are simply looking for the traditional probablitities between those states. What's the likelhood that they will co-occur.\n",
    "* The purpose of this tutorial is to help you understand how to create a neaural embedding for any group of discrete and co-occurring states. In addition, illustrate the usefulness of Word2vec in grouping vectors of similar words together in vectorspace. \n",
    "* Word2vec can make highly accurate guesses about a word’s meaning based on past appearances. \n",
    "* Through the guesses word association can be established with other words (e.g. “man” is to “boy” what “woman” is to “girl”) creating clusters or cluster documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://image.slidesharecdn.com/20151125hiratewebdb2015-151127073513-lva1-app6891/95/recommender-system-with-distributed-representation-4-638.jpg?cb=1448609942\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://image.slidesharecdn.com/20151125hiratewebdb2015-151127073513-lva1-app6891/95/recommender-system-with-distributed-representation-4-638.jpg?cb=1448609942\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Skip gram deep learning via word2vec's \"skip-gram and CBOW models\", using hierarchical softmax\n",
    "* A well trained set of word vectors will place similar words close to each other in that space.\n",
    "* Similar things and ideas are shown to be “close”and is just the basis of many associations that Word2vec can learn.\n",
    "* Word2vec can gauge relations between words of one language, and map them to another\n",
    "* So, depending on the approach (starting from context word, try to predict target word or from target word predict the context), a neural network is trained. The network trained can either be a softmax or negative sample with input-hidden layer-output. The weights between the input and the hidden layer are a W matrix and between hidden layer and output there is a W ' matrix. So after training the W matrix are the rapresentation of words as vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.tensorflow.org/images/softmax-nplm.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "Image(url= \"https://www.tensorflow.org/images/softmax-nplm.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://image.slidesharecdn.com/tdscspeech-151114010319-lva1-app6891/95/science-in-text-mining-8-638.jpg?cb=1447463123 \"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "Image(url= \"https://image.slidesharecdn.com/tdscspeech-151114010319-lva1-app6891/95/science-in-text-mining-8-638.jpg?cb=1447463123 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://image.slidesharecdn.com/whatisword2vec-150901142702-lva1-app6891/95/what-is-word2vec-21-638.jpg?cb=1441117721 \"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "Image(url= \"https://image.slidesharecdn.com/whatisword2vec-150901142702-lva1-app6891/95/what-is-word2vec-21-638.jpg?cb=1441117721 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import csv\n",
    "import os \n",
    "import gensim\n",
    "import logging\n",
    "import re as regex\n",
    "import numpy as np\n",
    "import plotly\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from gensim import utils \n",
    "\n",
    "#import corpus \n",
    "from collections import Counter\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import word2vec \n",
    "\n",
    "\n",
    "# import packages for text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# feature engineering (words to vectors)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#class ModifiedTaggedBrownCorpus(object):\n",
    "\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec\n",
    "from time import time\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "#from gensim.models import  \n",
    "from gensim import models, corpora\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In getting started you first must ensure that you upload gensim, and listed below are the necessary steps you must follow to upload gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library\n",
    "\n",
    "#### 1. Install pip\n",
    "\n",
    "* A) Start a command prompt \n",
    "\n",
    "Click Start, click All Programs, and then click Accessories.\n",
    "Right-click Command prompt, and then click Run as administrator.\n",
    "If the User Account Control dialog box appears, confirm that the action it displays is what you want, and then click Continue.\n",
    "\n",
    "* B) Download get-pip.py, being careful to save it as a .py file rather than .txt. Then, run it from the command prompt.\n",
    "\n",
    "python get-pip.py\n",
    "\n",
    "Download get-pip.py, and save it as a get-pip.py(not get-pip.txt).\n",
    "\n",
    "Run it from the command prompt.\n",
    "\n",
    "python get-pip.py\n",
    "\n",
    "#### 2. Install word2vec\n",
    "\n",
    "pip install word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once gensim fuctions have been installed library, then you can proceed to preparig and importing the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Input\n",
    "* During the initial preparation gensim’s word2vec expects a sequence of sentences as its input. Each sentence a list of words (utf8 strings). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # import modules and set up logging\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508731"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload data\n",
    "texts = []\n",
    "r = csv.reader(open(r'C:\\Users\\User\\Desktop\\670\\7_Topic_Modeling\\data\\text8.csv'))\n",
    "for i in r:\n",
    "    texts.append(i)  \n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable  x  indicates the list within the text, and the variable y, indicate the labels or target words to be used during the gensim word2vec proccess.  In order to conduct some true analysis of the corpus, the two datasets must be joined, and theis proess is achieved through a process called concatenating. In the tutorial the aformentioned processes aere exccuted below respectfully.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = X + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the datasets have been joined, the next thing to do would be to tokenize each word before passing them to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getnewargs__',\n",
       " '__getslice__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_formatter_field_name_split',\n",
       " '_formatter_parser',\n",
       " 'capitalize',\n",
       " 'center',\n",
       " 'count',\n",
       " 'decode',\n",
       " 'encode',\n",
       " 'endswith',\n",
       " 'expandtabs',\n",
       " 'find',\n",
       " 'format',\n",
       " 'index',\n",
       " 'isalnum',\n",
       " 'isalpha',\n",
       " 'isdigit',\n",
       " 'islower',\n",
       " 'isspace',\n",
       " 'istitle',\n",
       " 'isupper',\n",
       " 'join',\n",
       " 'ljust',\n",
       " 'lower',\n",
       " 'lstrip',\n",
       " 'partition',\n",
       " 'replace',\n",
       " 'rfind',\n",
       " 'rindex',\n",
       " 'rjust',\n",
       " 'rpartition',\n",
       " 'rsplit',\n",
       " 'rstrip',\n",
       " 'split',\n",
       " 'splitlines',\n",
       " 'startswith',\n",
       " 'strip',\n",
       " 'swapcase',\n",
       " 'title',\n",
       " 'translate',\n",
       " 'upper',\n",
       " 'zfill']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk_Tokenize = [nltk.word_tokenize(sent.decode('UTF-8')) for sent in corpus]\n",
    "nltk_Tokenize = [u' nltk_Tokenize']\n",
    "nltk_Tokenize = ('utf-8')\n",
    "dir (nltk_Tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a gensim model and is all you need to do to create your word2vec model. Note, the model that's saved is important in that it can be used to upload future word2vec model in which can save you a lot loading time as well as with cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-13 17:04:18,084 : INFO : collecting all words and their counts\n",
      "2017-11-13 17:04:18,084 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings).First item here is instead plain <type 'str'>.\n",
      "2017-11-13 17:04:18,099 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-13 17:04:18,102 : INFO : collected 5 word types from a corpus of 5 raw words and 5 sentences\n",
      "2017-11-13 17:04:18,105 : INFO : min_count=1 retains 5 unique words (drops 0)\n",
      "2017-11-13 17:04:18,107 : INFO : min_count leaves 5 word corpus (100% of original 5)\n",
      "2017-11-13 17:04:18,109 : INFO : deleting the raw counts dictionary of 5 items\n",
      "2017-11-13 17:04:18,111 : INFO : sample=0.001 downsamples 5 most-common words\n",
      "2017-11-13 17:04:18,115 : INFO : downsampling leaves estimated 0 word corpus (7.6% of prior 5)\n",
      "2017-11-13 17:04:18,117 : INFO : estimated required memory for 5 words and 32 dimensions: 3780 bytes\n",
      "2017-11-13 17:04:18,118 : INFO : resetting layer weights\n",
      "2017-11-13 17:04:18,125 : INFO : training model with 4 workers on 5 vocabulary and 32 features, using sg=0 hs=0 sample=0.001 negative=5\n",
      "2017-11-13 17:04:18,128 : INFO : expecting 5 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-11-13 17:04:18,138 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-13 17:04:18,140 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-13 17:04:18,141 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-13 17:04:18,144 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-13 17:04:18,145 : INFO : training on 25 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-11-13 17:04:18,148 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec as word2vec\n",
    "models = gensim.models.Word2Vec(nltk_Tokenize, size=32, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model\n",
    "- Word2vec accepts several parameters that affect both training speed and quality.\n",
    "- One is the pruning of the internal dictionary. Words that appear only once or twice in a billion-word corpus do not enough data to make any meaningful training and should be ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-13 17:04:18,234 : INFO : saving Word2Vec object under models, separately None\n",
      "2017-11-13 17:04:18,234 : INFO : not storing attribute syn0norm\n",
      "2017-11-13 17:04:18,234 : INFO : not storing attribute cum_table\n"
     ]
    }
   ],
   "source": [
    "# an empty model, no training yet\n",
    "models.save('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-13 17:04:18,375 : INFO : loading projection weights from C:\\Users\\User\\Desktop\\670\\7_Topic_Modeling\\data\\text8.csv\n",
      "2017-11-13 17:06:53,046 : INFO : loaded (508730L, 200L) matrix from C:\\Users\\User\\Desktop\\670\\7_Topic_Modeling\\data\\text8.csv\n"
     ]
    }
   ],
   "source": [
    " # continue training with the loaded model!\n",
    "from gensim import models\n",
    "models = models.Word2Vec.load_word2vec_format((r'C:\\Users\\User\\Desktop\\670\\7_Topic_Modeling\\data\\text8.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class BrownCorpus(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            fname = os.path.join(self.dirname, fname)\n",
    "            if not os.path.isfile(fname):\n",
    "                    continue\n",
    "            for line in open(fname):\n",
    "                # each file line is a single sentence in the Brown corpus\n",
    "                # each token is WORD/POS_TAG\n",
    "                token_tags = [t.split('/') for t in line.split() if len(t.split('/')) == 2]\n",
    "                # ignore words with non-alphabetic tags like \",\", \"!\" etc (punctuation, weird stuff)\n",
    "                words = [\"%s/%s\" % (token.lower(), tag[:2]) for token, tag in token_tags if tag[:2].isalpha()]\n",
    "                if not words:  # don't bother sending out empty sentences\n",
    "                    continue \n",
    "                    yield gensim.models.doc2vec.TaggedDocument(words,['%s_SENT_%s' % (fname, item_no)])\n",
    "\n",
    "               \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you are trying to determine to similarities of words, you would enter the following code and it should yield all words similiar to the target word, one hot word or the word the is  seen in parenthesis. Note, that it is here where word2vec will utilize the skip gram method to shorten the distance bewteen the target word and the words of those located in vectors in order to combine more similiar words. Also, be aware when excuting most similiar words  it will take a little while to execute because the corpus is relatively large. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models.most_simliar('Queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Anytime you want to see a the number type in the below information.  For instance, in the case of the model we set the size for 32, which means once you execute the below code there will appear an array of 32 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = models.most_simliar[(vectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: do to technical issues I was unable to perform the last to steps to this tutorial; however, barring the last two steps not being executed this concludes this tutorial on word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "* 1) https://github.com/danielfrg/word2vec\n",
    "* 2) https://rare-technologies.com/word2vec-tutorial\n",
    "* 3) https://en.wikipedia.org/wiki/Word2vec\n",
    "* 4) https://rare-technologies.com/word2vec-tutorial/#memory\n",
    "* 5) https://deeplearning4j.org/word2vec.html#intro\n",
    "* 6) https://deeplearning4j.org/word2vec#code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
